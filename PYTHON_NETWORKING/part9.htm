<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta/><title>Chapter 8: Network Programming with AsyncIO</title><link href="navigation.css"/><link href="document.css"/></head><body><p><a href="part8.htm">&lt; Назад</a> | <a href="index.html">Содержимое</a> | <a href="part10.htm">Далее &gt;</a></p><p><a>CHAPTER 8: NETWORK PROGRAMMING WITH ASYNCIO</a><a>&zwnj;</a></p><h2><a>Introducing Asynchronous Programming</a></h2><p><a>What is Asynchronous Programming?</a></p><p>Asynchronous programming is a paradigm that allows operations to execute without blocking the execution flow of your program. This is particularly useful in networking where I/O operations, such as reading and writing data to a network, are common and can potentially take a significant amount of time due to factors like network latency.</p><p>In traditional synchronous programming, the execution flow of your program is blocked while an I/O operation is performed. This means that if you send a request over the network, your program will stop and wait for the response before it continues. This can lead to inefficient use of resources, especially in scenarios where you are dealing with multiple network connections simultaneously, as is often the case in network programming.</p><p>Asynchronous programming addresses this issue by allowing the program to continue executing other tasks while waiting for the I/O operation to complete. This can lead to more efficient use of resources and improved responsiveness, especially in applications dealing with multiple simultaneous network connections.</p><p>Python supports asynchronous programming through the asyncio library, which provides an event loop, coroutines, and tasks to help you write asynchronous code. The event loop is the core of every asyncio application, and it can handle multiple I/O-bound tasks concurrently.</p><p><a>Why Async Programming for Networking?</a></p><p>The need for asynchronous programming in networking is paramount, especially when building applications like web servers, chat servers, or any service that requires handling multiple client requests simultaneously. With asynchronous programming, you can build more scalable and responsive applications.</p><p>In network automation tasks, for example, you might need to push configuration changes to hundreds or even thousands of network devices. With synchronous programming, you would have to wait for each device to respond before moving on to the next, which could be time- consuming. With asynchronous programming, you can send out all the changes simultaneously and then process the responses as they come in, leading to faster and more efficient operations.</p><p>Asynchronous programming is not just limited to networking but is also widely used in other areas such as web scraping, API integrations, and any scenario where the application has to deal with potentially slow I/O operations. However, it&#39;s worth noting that asynchronous programming can be more complex than synchronous programming due to the concurrent nature of tasks and the potential for race conditions. Care must be taken to properly synchronize tasks where necessary.</p><h2><a>AsyncIO Library</a></h2><p><a>Understanding AsyncIO</a></p><p>AsyncIO is a Python library that provides support for asynchronous I/O using coroutines and an event loop. It&#39;s part of the Python Standard Library, which means it comes with Python and doesn&#39;t require separate installation. However, AsyncIO is a relatively recent addition to Python and is only available in Python 3.4 and later. The library has been continuously updated and improved in subsequent Python versions, with notable enhancements in Python 3.5, 3.6, and 3.7.</p><p>AsyncIO provides several key features:</p><p>Coroutines: These are special functions that can be paused and resumed, allowing you to write asynchronous code in a synchronous style. In Python, you define a coroutine using the async def syntax. Inside a coroutine, you can use the await keyword to pause the execution until the awaited object is done.</p><p>Tasks: These are a subclass of Future that wraps coroutines. A Task is responsible for executing a coroutine object in an event loop. When a Task is created, it schedules the execution of its coroutine and can be awaited to get its result.</p><p>Event Loop: This is the core of every AsyncIO application. The event loop schedules and executes tasks and callbacks, handles I/O events, and provides mechanisms for synchronization and communication between tasks.</p><p>Futures: These are objects that represent the result of a computation that may not have completed yet. In AsyncIO, you typically don&#39;t work directly with Futures, as the library prefers using Tasks and coroutines.</p><p>Streams: AsyncIO provides high-level support for managing network I/O using streams. The library provides reader and writer objects for handling streaming I/O in a coroutine-friendly manner.</p><p><a>Installing AsyncIO</a></p><p>As mentioned earlier, AsyncIO is part of the Python Standard Library, so you don&#39;t need to install it separately. To use AsyncIO in your Python program, you just need to import it:</p><p><br/></p><p>import                         asyncio                         </p><p>AsyncIO presents a high-level, intuitive API for handling asynchronous programming in Python. While the inherently intricate nature of asynchronous programming introduces a learning curve, it becomes an incredibly potent tool for developing efficient and reactive network programs once you&#39;re accustomed to it.</p><p><a>Using AsyncIO to Run Coroutine</a></p><p>Let us start with a simple asyncio example. We&#39;ll create a coroutine that sleeps for a given amount of time and then prints a message.</p><p><br/></p><p>import asyncio</p><p>async def say_after(delay, what): await asyncio.sleep(delay) print(what)</p><p>asyncio.run(say_after(1, &#39;Hello, Asyncio!&#39;))</p><p>import asyncio</p><p>async def say_after(delay, what): await asyncio.sleep(delay) print(what)</p><p>asyncio.run(say_after(1, &#39;Hello, Asyncio!&#39;))</p><p/><p>import asyncio</p><p>async def say_after(delay, what): await asyncio.sleep(delay) print(what)</p><p>asyncio.run(say_after(1, &#39;Hello, Asyncio!&#39;))</p><p>In the above snippet, say_after is a coroutine that uses the await keyword to pause its execution for a certain delay. After the delay, it resumes execution and prints the message. The asyncio.run() function is a convenience function for running a coroutine and returning its result. It creates a new event loop, runs the given coroutine, and closes the loop.</p><p><a>Running Multiple Coroutines</a></p><p>Now let us move on to a slightly more complex example: running multiple coroutines concurrently. Below is how you can do it with asyncio:</p><p><br/></p><p>import asyncio</p><p>async def say_after(delay, what): await asyncio.sleep(delay) print(what)</p><p>async def main():</p><p>task1 = asyncio.create_task(say_after(1, &#39;Hello&#39;)) task2 = asyncio.create_task(say_after(2, &#39;Asyncio&#39;)) await task1</p><p>await task2</p><p>asyncio.run(main())</p><p>import asyncio</p><p>async def say_after(delay, what): await asyncio.sleep(delay) print(what)</p><p>async def main():</p><p>task1 = asyncio.create_task(say_after(1, &#39;Hello&#39;)) task2 = asyncio.create_task(say_after(2, &#39;Asyncio&#39;)) await task1</p><p>await task2</p><p>asyncio.run(main())</p><p/><p>import asyncio</p><p>async def say_after(delay, what): await asyncio.sleep(delay) print(what)</p><p>async def main():</p><p>task1 = asyncio.create_task(say_after(1, &#39;Hello&#39;)) task2 = asyncio.create_task(say_after(2, &#39;Asyncio&#39;)) await task1</p><p>await task2</p><p>asyncio.run(main())</p><p>In the above program, the main coroutine creates two tasks that run the say_after coroutine with different arguments. It then waits for both tasks to complete using the await keyword. The asyncio.create_task() function schedules a coroutine to run as a task and returns a Task object.</p><p>When you run this program, you&#39;ll see that &#39;Hello&#39; is printed after 1 second and &#39;Asyncio&#39; is</p><p>printed after 2 seconds. However, because the tasks are running concurrently, the total runtime of the program is only 2 seconds, not 3. This is the power of asynchronous programming: it allows you to run multiple I/O-bound tasks concurrently, increasing the efficiency of your program.</p><p><a>Create Echo Server and Client using AsyncIO</a></p><p>AsyncIO provides a set of high-level APIs for building network services, including a TCP server and client. Let us take a look at how you can create a simple echo server and client using AsyncIO.</p><p>First, below is how you can create an echo server:</p><p><br/></p><p>import asyncio</p><p>async def echo_handler(reader, writer): data = await reader.read(100) message = data.decode()</p><p>addr = writer.get_extra_info(&#39;peername&#39;) print(f&quot;Received {message} from {addr}&quot;) print(f&quot;Send: {message}&quot;) writer.write(data)</p><p>await writer.drain() print(&quot;Closing the connection&quot;) writer.close()</p><p>async def main():</p><p>server = await asyncio.start_server( echo_handler, &#39;127.0.0.1&#39;, 8888)</p><p>addr = server.sockets[0].getsockname() print(f&#39;Serving on {addr}&#39;)</p><p>async with server:</p><p>await server.serve_forever() asyncio.run(main())</p><p>import asyncio</p><p>async def echo_handler(reader, writer): data = await reader.read(100) message = data.decode()</p><p>addr = writer.get_extra_info(&#39;peername&#39;) print(f&quot;Received {message} from {addr}&quot;) print(f&quot;Send: {message}&quot;) writer.write(data)</p><p>await writer.drain() print(&quot;Closing the connection&quot;) writer.close()</p><p>async def main():</p><p>server = await asyncio.start_server( echo_handler, &#39;127.0.0.1&#39;, 8888)</p><p>addr = server.sockets[0].getsockname() print(f&#39;Serving on {addr}&#39;)</p><p>async with server:</p><p>await server.serve_forever() asyncio.run(main())</p><p/><p>import asyncio</p><p>async def echo_handler(reader, writer): data = await reader.read(100) message = data.decode()</p><p>addr = writer.get_extra_info(&#39;peername&#39;) print(f&quot;Received {message} from {addr}&quot;) print(f&quot;Send: {message}&quot;) writer.write(data)</p><p>await writer.drain() print(&quot;Closing the connection&quot;) writer.close()</p><p>async def main():</p><p>server = await asyncio.start_server( echo_handler, &#39;127.0.0.1&#39;, 8888)</p><p>addr = server.sockets[0].getsockname() print(f&#39;Serving on {addr}&#39;)</p><p>async with server:</p><p>await server.serve_forever() asyncio.run(main())</p><p><img src="Image_022.png"/></p><p><br/></p><p>The echo_handler coroutine reads data from a client, sends the same data back to the client, and then closes the connection. The asyncio.start_server() function starts a TCP server that listens for connections on 127.0.0.1:8888 and uses echo_handler to handle the connections.</p><p>Next, below is how you can create an echo client that connects to the echo server and sends a message:</p><p><br/></p><p>import asyncio</p><p>async def echo_client(message):</p><p>reader, writer = await asyncio.open_connection(&#39;127.0.0.1&#39;, 8888) print(f&#39;Send: {message}&#39;)</p><p>writer.write(message.encode()) data = await reader.read(100) print(f&#39;Received: {data.decode()}&#39;) print(&#39;Closing the connection&#39;) writer.close()</p><p>message = &#39;Hello, AsyncIO!&#39;</p><p>asyncio.run(echo_client(message))</p><p>import asyncio</p><p>async def echo_client(message):</p><p>reader, writer = await asyncio.open_connection(&#39;127.0.0.1&#39;, 8888) print(f&#39;Send: {message}&#39;)</p><p>writer.write(message.encode()) data = await reader.read(100) print(f&#39;Received: {data.decode()}&#39;) print(&#39;Closing the connection&#39;) writer.close()</p><p>message = &#39;Hello, AsyncIO!&#39;</p><p>asyncio.run(echo_client(message))</p><p/><p>import asyncio</p><p>async def echo_client(message):</p><p>reader, writer = await asyncio.open_connection(&#39;127.0.0.1&#39;, 8888) print(f&#39;Send: {message}&#39;)</p><p>writer.write(message.encode()) data = await reader.read(100) print(f&#39;Received: {data.decode()}&#39;) print(&#39;Closing the connection&#39;) writer.close()</p><p>message = &#39;Hello, AsyncIO!&#39;</p><p>asyncio.run(echo_client(message))</p><p><br/></p><p>The echo_client coroutine connects to the server, sends a message, receives the echoed message, and then closes the connection. The asyncio.open_connection() function opens a TCP connection to the server.</p><p>When you run the server and client programs, you should see that the client sends a message to the server, and the server echoes the message back to the client.</p><p><a>Concurrent Data Processing using AsyncIO</a></p><p>Concurrency is one of the main reasons to use asynchronous programming. Let us see how we can use asyncio for concurrent data processing.</p><p>Consider a scenario where we need to download several web pages and process the HTML data. If we did this synchronously, we would have to wait for each page to be downloaded and processed before moving on to the next one, which could be very slow if there are a lot of pages.</p><p>With asyncio, we can download and process the pages concurrently, which could significantly speed up the task. Below is how we can do this:</p><p>import aiohttp import asyncio</p><p>async def download_page(session, url): async with session.get(url) as response:</p><p>return await response.text() async def process_page(page):</p><p># Simulate a CPU-bound task by sleeping await asyncio.sleep(1)</p><p>print(f&#39;Page length: {len(page)}&#39;) async def main():</p><p>urls = [&#39;http://example.com&#39; for _ in range(10)]</p><p><br/></p><p>async with aiohttp.ClientSession() as session: tasks = []</p><p>for url in urls: tasks.append(download_page(session, url))</p><p>pages = await asyncio.gather(*tasks) tasks = []</p><p>for page in pages: tasks.append(process_page(page))</p><p>await asyncio.gather(*tasks) # Python 3.7+ asyncio.run(main())</p><p>In the above program, we create a list of tasks for downloading the pages and then use asyncio.gather() to run the tasks concurrently. We then do the same for processing the pages.</p><p><img src="Image_023.png"/></p><p>The download_page() coroutine uses the aiohttp library to download a web page asynchronously. The process_page() coroutine simulates a CPU-bound task by sleeping for one second.</p><p>You can see how easy it is to run tasks concurrently with asyncio. The key is to create coroutines for the tasks and then use asyncio.gather() or asyncio.wait() to run them concurrently. Also, note that we use aiohttp.ClientSession() to create an HTTP session. This is recommended for making multiple requests because it allows aiohttp to reuse the TCP connection, which can improve performance.</p><p>Remember that asyncio is single-threaded, so it&#39;s best suited for IO-bound tasks. If you have CPU-bound tasks that could benefit from parallel execution, you might want to use a multi- threaded or multi-process approach instead.</p><h2><a>AsyncIO for IO Bound Operations</a></h2><p>asyncio can be used in a variety of applications to facilitate efficient IO-bound operations. Following are a few popular use cases with other Python libraries:</p><p>●   Web Scraping: asyncio can be used with libraries such as aiohttp or httpx to perform concurrent web requests, significantly speeding up the process of web scraping or API data collection.</p><p>●   Web Servers: asyncio is often used to build efficient, scalable web servers. Libraries such as aiohttp and fastapi can be used to build servers that can handle many concurrent connections, which is crucial for high-load web applications.</p><p>●     Websockets: Libraries like websockets can leverage asyncio to build real-time applications, like chat servers, where multiple connections need to be open and managed concurrently.</p><p>●   Databases: Asynchronous database libraries such as aiomysql, aiopg and aioredis allow non-blocking database operations, which can significantly improve the performance of IO-bound applications that interact heavily with databases.</p><p>●   Tasks and Job Scheduling: Libraries like huey and asyncio can be combined to create asynchronous task queues and job schedulers, which can offload long running or IO- bound tasks from the main application thread and process them asynchronously.</p><p>●   Networking: Libraries such as aiodns for asynchronous DNS resolutions, or aioftp for asynchronous FTP clients can be used with asyncio for efficient network programming.</p><p>●   File IO: Although asyncio is mainly used for network IO, it can also be used for file IO with the aiofiles library. This can be useful for applications that need to handle many file operations concurrently.</p><p>Each of these library combinations extend the functionality of asyncio, providing specific tools for tasks such as HTTP requests, database connections, and more. Each library typically uses the same asyncio patterns (like tasks, coroutines, and futures), but provides additional functionality for their specific domain. For example, aiohttp provides an interface for making HTTP requests and responses, while aiomysql provides tools for connecting to and querying a MySQL database.</p><p><a>Web Scraping with aiohttp</a></p><p>Aiohttp is a library for making HTTP requests, and it can be used with asyncio to make these requests concurrently.</p><p><br/></p><p>import aiohttp import asyncio</p><p>async def fetch(session, url):</p><p>import aiohttp import asyncio</p><p>async def fetch(session, url):</p><p/><p>import aiohttp import asyncio</p><p>async def fetch(session, url):</p><p>async with session.get(url) as response: return await response.text()</p><p>async def main():</p><p>urls = [&#39;http://python.org&#39;, &#39;http://google.com&#39;] async with aiohttp.ClientSession() as session: tasks = [fetch(session, url) for url in urls]</p><p>htmls = await asyncio.gather(*tasks) for url, html in zip(urls, htmls):</p><p>print(f&#39;{url} page length: {len(html)}&#39;)</p><p>asyncio.run(main())</p><p>async with session.get(url) as response: return await response.text()</p><p>async def main():</p><p>urls = [&#39;http://python.org&#39;, &#39;http://google.com&#39;] async with aiohttp.ClientSession() as session: tasks = [fetch(session, url) for url in urls]</p><p>htmls = await asyncio.gather(*tasks) for url, html in zip(urls, htmls):</p><p>print(f&#39;{url} page length: {len(html)}&#39;)</p><p>asyncio.run(main())</p><p/><p>async with session.get(url) as response: return await response.text()</p><p>async def main():</p><p>urls = [&#39;http://python.org&#39;, &#39;http://google.com&#39;] async with aiohttp.ClientSession() as session: tasks = [fetch(session, url) for url in urls]</p><p>htmls = await asyncio.gather(*tasks) for url, html in zip(urls, htmls):</p><p>print(f&#39;{url} page length: {len(html)}&#39;)</p><p>asyncio.run(main())</p><p>Below is a step-by-step breakdown of the above web scraping program:</p><p>The first two lines import the necessary libraries, aiohttp for making HTTP requests and asyncio for managing asynchronous tasks. The fetch function is an asynchronous function that takes a session and a url as arguments. This function is responsible for making a GET request to the provided URL and returning the text of the response. The async with construct is used to manage the context of the HTTP request, ensuring that resources are properly cleaned up when the request is finished.</p><p>The main function is the entry point of the program. It creates an asynchronous context for an aiohttp.ClientSession, which is a class for making HTTP requests. This session object is then passed to multiple instances of the fetch function (one for each URL), which are stored as tasks. The asyncio.gather function is used to run all the fetch tasks concurrently and wait for all of them to complete. It returns a list of results corresponding to the return values of the fetch functions.</p><p>After all the tasks have completed, the function iterates over the URLs and the corresponding HTML responses, printing the length of each response. Finally, the asyncio.run function is used to execute the main coroutine. This function creates a new event loop, runs the given coroutine, and closes the loop. The event loop is the core of every asyncio application and is responsible for executing coroutines and scheduling callbacks.</p><p><a>Web Server with aiohttp</a></p><p>You can use aiohttp to create an async web server.</p><p><br/></p><p>from aiohttp import web</p><p>async def handle(request):</p><p>from aiohttp import web</p><p>async def handle(request):</p><p/><p>from aiohttp import web</p><p>async def handle(request):</p><p>return web.Response(text=&quot;Hello, world&quot;) app = web.Application() app.add_routes([web.get(&#39;/&#39;, handle)])</p><p>web.run_app(app)</p><p>return web.Response(text=&quot;Hello, world&quot;) app = web.Application() app.add_routes([web.get(&#39;/&#39;, handle)])</p><p>web.run_app(app)</p><p/><p>return web.Response(text=&quot;Hello, world&quot;) app = web.Application() app.add_routes([web.get(&#39;/&#39;, handle)])</p><p>web.run_app(app)</p><p>This above program creates a simple web server that responds to GET requests with &quot;Hello, world&quot;.</p><p>handle(request): This coroutine is the request handler. It gets called whenever a GET request is received. It returns a Response object with the text &quot;Hello, world&quot;.</p><p>web.Application(): This creates a new aiohttp web application.</p><p>app.add_routes([web.get(&#39;/&#39;, handle)]): This adds a route to the application. The route is a URL path and a coroutine function (the request handler) that gets called when a GET request is received at that path.</p><p>web.run_app(app): This starts the aiohttp web server with the application.</p><p>These examples demonstrate how asyncio can be used with other libraries to perform various tasks concurrently.</p><p><a>Database Access with aiomysql</a></p><p>Aiomysql provides an async interface for interacting with MySQL databases.</p><p><br/></p><p>import asyncio import aiomysql async def main():</p><p>pool = await aiomysql.create_pool(host=&#39;127.0.0.1&#39;, port=3306,</p><p>user=&#39;user&#39;, password=&#39;password&#39;, db=&#39;db&#39;, loop=loop)</p><p>async with pool.acquire() as conn: async with conn.cursor() as cur:</p><p>await cur.execute(&quot;SELECT 42;&quot;)</p><p>print(await cur.fetchone())</p><p>import asyncio import aiomysql async def main():</p><p>pool = await aiomysql.create_pool(host=&#39;127.0.0.1&#39;, port=3306,</p><p>user=&#39;user&#39;, password=&#39;password&#39;, db=&#39;db&#39;, loop=loop)</p><p>async with pool.acquire() as conn: async with conn.cursor() as cur:</p><p>await cur.execute(&quot;SELECT 42;&quot;)</p><p>print(await cur.fetchone())</p><p/><p>import asyncio import aiomysql async def main():</p><p>pool = await aiomysql.create_pool(host=&#39;127.0.0.1&#39;, port=3306,</p><p>user=&#39;user&#39;, password=&#39;password&#39;, db=&#39;db&#39;, loop=loop)</p><p>async with pool.acquire() as conn: async with conn.cursor() as cur:</p><p>await cur.execute(&quot;SELECT 42;&quot;)</p><p>print(await cur.fetchone())</p><p>loop = asyncio.get_event_loop()</p><p>loop.run_until_complete(main())</p><p>loop = asyncio.get_event_loop()</p><p>loop.run_until_complete(main())</p><p/><p>loop = asyncio.get_event_loop()</p><p>loop.run_until_complete(main())</p><p>This above program queries a MySQL database for the number 42.</p><p>aiomysql.create_pool(): This coroutine creates a connection pool to a MySQL database. A connection pool is a cache of database connections that can be reused, which is more efficient than opening a new connection for each query.</p><p>pool.acquire(): This coroutine gets a connection from the pool.</p><p>conn.cursor(): This coroutine creates a new cursor object. A cursor is used to execute SQL statements.</p><p>cur.execute(&quot;SELECT 42;&quot;): This coroutine executes an SQL statement.</p><p>cur.fetchone(): This coroutine fetches the next row of a query result set, which is the number 42 in this case.</p><p><a>File IO with aiofiles</a></p><p>Aiofiles provides an async interface for file IO.</p><p><br/></p><p>import asyncio import aiofiles</p><p>async def write_file(data):</p><p>async with aiofiles.open(&#39;file.txt&#39;, &#39;w&#39;) as f: await f.write(data)</p><p>async def main():</p><p>await write_file(&#39;Hello, world!&#39;) asyncio.run(main())</p><p>import asyncio import aiofiles</p><p>async def write_file(data):</p><p>async with aiofiles.open(&#39;file.txt&#39;, &#39;w&#39;) as f: await f.write(data)</p><p>async def main():</p><p>await write_file(&#39;Hello, world!&#39;) asyncio.run(main())</p><p/><p>import asyncio import aiofiles</p><p>async def write_file(data):</p><p>async with aiofiles.open(&#39;file.txt&#39;, &#39;w&#39;) as f: await f.write(data)</p><p>async def main():</p><p>await write_file(&#39;Hello, world!&#39;) asyncio.run(main())</p><p>This above program writes &quot;Hello, world&quot; to a file.</p><p>aiofiles.open(): This coroutine opens a file. The &#39;w&#39; argument means the file is opened for writing.</p><p>f.write(data): This coroutine writes data to the file.</p><p>In all these given programs, asyncio.run() is used to start the asyncio event loop and schedule a coroutine to run. This is the main entry point for asyncio programs, and it takes care of creating</p><p>and cleaning up the event loop, so you don&#39;t have to do it manually.</p><h2><a>Summary</a></h2><p>This chapter provided an in-depth exploration of asynchronous programming in Python, focusing on the asyncio library. Asynchronous programming, as opposed to synchronous or sequential programming, allows for the execution of certain tasks simultaneously. This is critical in networking where tasks such as sending or receiving data packets, interacting with APIs, or querying databases may block execution, thus slowing down the entire process. Utilizing asyncio, we learned to develop concurrent tasks, allowing multiple operations to progress concurrently, significantly improving the efficiency of network programs.</p><p>We delved into the asyncio library, examining its features and practical applications. We learned to create and manage asyncio event loops, crucial for scheduling and executing coroutines. Using aiohttp, we performed concurrent web scraping. We also created a simple web server that could handle multiple requests concurrently. We then interfaced asyncio with aiomysql to perform concurrent database operations, demonstrating how asyncio can be used to prevent blocking during database access. Lastly, we explored aiofiles for asynchronous file operations, providing a way to handle file I/O that doesn&#39;t block the event loop.</p><p>The chapter wrapped up by demonstrating the compatibility of asyncio with other Python libraries, highlighting the versatility of asynchronous programming. We showed how aiohttp can be used for concurrent web requests, aiomysql for non-blocking database access, and aiofiles for non-blocking file I/O. Each of these libraries leverages asyncio&#39;s capabilities, extending the benefits of asynchronous programming to various domains. By integrating these libraries, we can build complex, efficient, and responsive network applications with Python.</p><p><br/></p><p><a href="part8.htm">&lt; Назад</a> | <a href="index.html">Содержимое</a> | <a href="part10.htm">Далее &gt;</a></p></body></html>
